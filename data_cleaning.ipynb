{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from ast import literal_eval\n",
    "from utilities.utils import dict2w\n",
    "from utilities.functions import keep_keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part one, csv cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dirty csv file.\n",
    "topic_df = pd.read_csv('data/stories.csv')\n",
    "\n",
    "# remove records with no transcripts\n",
    "topic_df = topic_df.where(topic_df['body']!=' ').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a translation dictionary for the topics\n",
    "### this is only for easier understanding in future references\n",
    "topics = topic_df['topic'].tolist()\n",
    "\n",
    "exceptions = []\n",
    "topic_set = set()\n",
    "clean_topics_temp = []\n",
    "\n",
    "for idx, tpc in enumerate(topics):\n",
    "    temp = []\n",
    "    try:\n",
    "        topic_list = literal_eval(tpc)\n",
    "        for topic in topic_list:\n",
    "            temp.append(topic)\n",
    "            topic_set.add(topic)\n",
    "    except:\n",
    "        exceptions.append(idx)\n",
    "    clean_topics_temp.append(temp)\n",
    "\n",
    "topics_unique = list(sorted(topic_set))\n",
    "topic_IDs = dict2w()\n",
    "\n",
    "for idx, topic in enumerate(topics_unique):\n",
    "    topic_IDs[idx] = topic\n",
    "\n",
    "with open(\"data/topic_ids.json\", \"w\") as outfile:\n",
    "    json.dump(topic_IDs, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# check if there's any record with problematic topic\n",
    "print(exceptions)\n",
    "\n",
    "### since there wasn't any, there's no need to handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce the 'topic' feature of the dataset\n",
    "clean_topics = []\n",
    "for tpc in clean_topics_temp:\n",
    "    temp = [topic_IDs[t] for t in tpc]\n",
    "    clean_topics.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                body       topic\n",
      "0  hello and welcome to BBC News a woman who gave...      [4, 7]\n",
      "1  news now out of North Hollywood. A 14 yearold ...         [7]\n",
      "2  homelessness his city's greatest failure. That...   [4, 3, 2]\n",
      "3  Minneapolis police officer Kim Potter guilty o...      [4, 7]\n",
      "4  Judy an update now to the wildfires that wiped...      [7, 6]\n",
      "5  the Sierra Nevada. Makes you want to cozy up u...         [6]\n",
      "6  proposed emergency declaration for San Francis...      [4, 5]\n",
      "7  Holmes will not reach a verdict before Christm...         [4]\n",
      "8  year is the place to be. There's already a lot...         [7]\n",
      "9  that. In South Africa today, a farewell to a g...  [4, 13, 2]\n"
     ]
    }
   ],
   "source": [
    "# reproduce, save and print the dataset\n",
    "clean_df = {'body': topic_df['body'].tolist(),\n",
    "            'topic': clean_topics}\n",
    "clean_topics_df = pd.DataFrame(clean_df)\n",
    "clean_topics_df.to_csv('data/stories_clean.csv', index=False)\n",
    "print(clean_topics_df.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part two, News Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_dir = 'data/transcripts'\n",
    "transcripts_names = os.listdir(transcripts_dir)\n",
    "\n",
    "for t in transcripts_names:\n",
    "    if t[-5:]!='.json' or t=='transcripts.json':\n",
    "        transcripts_names.remove(t)\n",
    "\n",
    "transcripts = {}\n",
    "\n",
    "for trspt in transcripts_names:\n",
    "    t_id = trspt[:-5]\n",
    "    file_dir = os.path.join(transcripts_dir, trspt)\n",
    "    with open(file_dir, 'r') as infile:\n",
    "        transcript = json.load(infile)\n",
    "    transcript['id'] = t_id\n",
    "    transcript = keep_keys(transcript, ['id', 'text', 'words'])\n",
    "    transcripts[t_id] = transcript\n",
    "\n",
    "transcript_name = 'transcripts.json'\n",
    "with open(os.path.join(transcripts_dir, transcript_name), \"w\") as outfile:\n",
    "    json.dump(transcripts, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
