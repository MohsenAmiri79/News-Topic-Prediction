{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ast import literal_eval\n",
    "from utilities.utils import dict2w\n",
    "from utilities.functions import keep_keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part one, csv cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dirty csv file.\n",
    "topic_df = pd.read_csv('data/stories.csv')\n",
    "\n",
    "# remove records with no transcripts\n",
    "topic_df = topic_df.where(topic_df['body'] != ' ').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a translation dictionary for the topics\n",
    "# this is only for easier understanding in future references\n",
    "topics = topic_df['topic'].tolist()\n",
    "\n",
    "# initialize lists that we are gonna use\n",
    "exceptions = []\n",
    "topic_set = set()  # we use sets to enforce uniqueness\n",
    "clean_topics_temp = []\n",
    "\n",
    "for idx, tpc in enumerate(topics):\n",
    "    temp = []\n",
    "\n",
    "    # add each id to the set and to the topics list\n",
    "    try:\n",
    "        topic_list = literal_eval(tpc)\n",
    "        for topic in topic_list:\n",
    "            temp.append(topic)\n",
    "            topic_set.add(topic)\n",
    "    except:\n",
    "        # managing exceptions\n",
    "        exceptions.append(idx)\n",
    "\n",
    "    # append all topic ids to a list\n",
    "    clean_topics_temp.append(temp)\n",
    "\n",
    "# create and save the id translation dictionary\n",
    "topics_unique = list(sorted(topic_set))\n",
    "topic_IDs = dict2w()\n",
    "\n",
    "for idx, topic in enumerate(topics_unique):\n",
    "    topic_IDs[idx] = topic\n",
    "\n",
    "with open(\"data/topic_ids.json\", \"w\") as outfile:\n",
    "    json.dump(topic_IDs, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# check if there's any record with problematic topic\n",
    "print(exceptions)\n",
    "\n",
    "# since there wasn't any, there's no need to handle it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce the 'topic' feature of the dataset\n",
    "num_topics = len(topic_IDs)\n",
    "clean_topics = []\n",
    "for tpc in clean_topics_temp:\n",
    "    temp = [topic_IDs[t] for t in tpc]\n",
    "    cleaned_topic = np.zeros(num_topics, dtype=np.int16)\n",
    "    for t in temp:\n",
    "        cleaned_topic[int(t)] = 1\n",
    "    clean_topics.append(list(cleaned_topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                body  \\\n",
      "0  hello and welcome to BBC News a woman who gave...   \n",
      "1  news now out of North Hollywood. A 14 yearold ...   \n",
      "2  homelessness his city's greatest failure. That...   \n",
      "3  Minneapolis police officer Kim Potter guilty o...   \n",
      "4  Judy an update now to the wildfires that wiped...   \n",
      "5  the Sierra Nevada. Makes you want to cozy up u...   \n",
      "6  proposed emergency declaration for San Francis...   \n",
      "7  Holmes will not reach a verdict before Christm...   \n",
      "8  year is the place to be. There's already a lot...   \n",
      "9  that. In South Africa today, a farewell to a g...   \n",
      "\n",
      "                                           topic  \n",
      "0  [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
      "2  [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "3  [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
      "4  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
      "5  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "6  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "7  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "8  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
      "9  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  \n"
     ]
    }
   ],
   "source": [
    "# reproduce, save and print the dataset\n",
    "clean_df = {'body': topic_df['body'].tolist(),\n",
    "            'topic': clean_topics}\n",
    "clean_topics_df = pd.DataFrame(clean_df)\n",
    "clean_topics_df.to_csv('data/stories_clean.csv', index=False)\n",
    "print(clean_topics_df.head(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part two, News Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the transcripts\n",
    "transcripts_dir = 'data/transcripts'\n",
    "transcripts_names = os.listdir(transcripts_dir)\n",
    "\n",
    "for t in transcripts_names:\n",
    "    if t[-5:] != '.json' or t == 'transcripts.json':\n",
    "        transcripts_names.remove(t)\n",
    "\n",
    "# initialize the transcripts dictionary\n",
    "transcripts = {}\n",
    "\n",
    "# iterate over transcript file\n",
    "for trspt in transcripts_names:\n",
    "    # find the transcript's ID\n",
    "    t_id = trspt[:-5]\n",
    "\n",
    "    # read the transcript\n",
    "    file_dir = os.path.join(transcripts_dir, trspt)\n",
    "    with open(file_dir, 'r') as infile:\n",
    "        transcript = json.load(infile)\n",
    "\n",
    "    # remove any extra information\n",
    "    transcript = keep_keys(transcript, ['text', 'words'])\n",
    "\n",
    "    # add the new transcript to the dictionary\n",
    "    transcripts[t_id] = transcript\n",
    "\n",
    "# save the transcripts\n",
    "transcript_name = 'transcripts.json'\n",
    "with open(os.path.join(transcripts_dir, transcript_name), \"w\") as outfile:\n",
    "    json.dump(transcripts, outfile, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
